---
jupyter:
  jupytext:
    text_representation:
      extension: .Rmd
      format_name: rmarkdown
      format_version: '1.2'
      jupytext_version: 1.13.8
  kernelspec:
    display_name: Python 3
    language: python
    name: python3
---

```{python tags=c()}
import numpy as np
import torch
from torchvision import models
from torchvision import transforms
from pathlib import Path
from tqdm import tqdm

import DatasetHandler as dh
import pooling
import regression as reg
import SFVQA as sfv
import VideoUtility as vu
```

```{python tags=c()}
def get_videoset_features(model, device, video_list: str, video_path: str, transform, dataset: str = 'LIVE') -> list:
    '''Get the VQA dataset video names and scores, set a feature extractor model,
       get frames of each video, then get features of each frame in videos
       
    :param video_list: list of video sequences name
    :param video_path: videos' directory
    :param transform: preprocessing pipeline
    :param dataset: name of the VQA dataset to extract the features from
    :param fine_tune: use fine-tuned model if True
    :return: videos' frames features of dimension and videos scores (DMOS)
    '''
    
    # Specify the layers to get style features from the feature extractor model
    layers = {
              # '0': 'conv1_1',
              '5': 'conv2_1', 
              # '10': 'conv3_1', 
              # '19': 'conv4_1',
              # '28': 'conv5_1'
              }
    # Convert the path string to a pathlib object
    video_path = Path(video_path)
    dataset = dataset.lower()
    # videos_frame_features = []
    for seq in tqdm(video_list):
        # Concatenate the video sequence name to the video directory to get the full video path
        vid_path = str(video_path / seq)
        if dataset == 'live':
            # Get frames of the video of the dimension 768 * 432 (LIVE VQA videos)
            vid_frames = vu.get_frames(vid_path, height=432, width=768)
        elif dataset == 'konvid1k':
            vid_frames = vu.get_frames(vid_path)
        # Get the features of all frames of the video
        frames_features = sfv.get_video_style_features(vid_frames, model, device, transform, layers)
        frames_features = np.array(frames_features)
        # videos_frame_features.append(frames_features)
        yield frames_features
    
    # return videos_frame_features, scores
```

```{python}
def load_save_features(feat_path, scores_path, features=None, scores=None, mode='save'):
    '''Load or save features
    
    :param feat_path: path to load from or save to the features
    :param scores_path: path to load from or save to the video scores
    :param features: features to be saved
    :param scores: scores to be saved
    :param mode: 'save' or 'load'
    :return: if mode is load, return features and scores
    '''
    
    if mode == 'save':
        if features is None or scores is None:
            return None
        else:
            np.save(feat_path, np.array(features))
            np.save(scores_path, scores)
            
    elif mode == 'load':
        return np.load(feat_path, allow_pickle=True), np.load(scores_path)
    else:
        return None
```

```{python}
# Entry point of the program
def main(mode):
    '''Run the whole process of VQA
    :param mode: 'save' if features have not already been extracted, 'load' otherwise
    '''
    # Specify the VQA dataset to train the model on
    dataset = 'KONVID1K'
    
    # Get the list of videos and corresponding scores and preprocessing module
    video_list, scores, transform, video_path = dh.get_videoset_info(dataset=dataset, frame_size=254)
    
    # Check if there is a GPU
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    # Set the frame features extractor model (VGG19)
    model = sfv.set_sf_model(device, fine_tune=True)
    
    # Get frame level features of all videos in the dataset
    if mode == 'save':
        videos_feats = get_videoset_features(model, device, video_list, video_path, transform, dataset)
        # Save the model
        # load_save_features('features.npy', 'scores.npy', features=videos_feats, scores=scores, mode=mode)
    elif mode == 'load':
        videos_feats, scores = load_save_features('features.npy', 'scores.npy', mode=mode)
    # Pool the frame level features to get video level features
    pooled_features = pooling.simple_pooling(videos_feats)
    # Train a regressor using video level features and indicate how well it predicts the scores
    # using various correlations
    reg.regression(pooled_features, scores, regression_method='svr', dataset=dataset)               
```

```{python tags=c()}
# Run the main function if current file is the script, not a module
if __name__ == "__main__":
    main('save')
```

```{python}

```

```{python}

```
